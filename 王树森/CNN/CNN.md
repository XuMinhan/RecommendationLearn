## 基础CNN

![img.png](img.png)

## 训练技巧

1. Dropout 正则化 (其他神经元需要弥补)
2. 数据增强
3. 预训练
4. 集成方法
5. 多任务学习
6. 批标准化
7. 梯度注入（Google Inception Net）
8. 跳跃连接（ResNet）


## 归一化

### 需要归一化的原因

![img_1.png](img_1.png)

Hessian 矩阵为损失函数的二阶导数, 可以用来评估梯度下降

特征值 和 特征向量 表示二阶导数的方向和程度

最大特征/最小特征 = condition number

表示 最大梯度/最小梯度 可以用来评估梯度下降是否合理

如果过大, 则表示 梯度下降的低拟合速度 甚至有可能不拟合

因此, 需要归一化



![img_2.png](img_2.png)

### 归一化 batch

采用 2 维进行归一化, 减少参数量 和 计算量


### 鞍点 局部极小值 全局极小值

这些内容的核心是：非凸优化中存在大量鞍点和局部极小值，找到全局极小值非常困难。训练的目标是至少逃过鞍点，找到一个较好的局部极小值。为此，需要：

慎重选择下降算法：SGD或Adam优于全梯度下降，能逃离鞍点。
慎重选择批量大小：小批量（32或更小）有助于逃离鞍点并提高泛化能力。
慎重选择初始化：好的初始化（如Xavier或预训练）可以避免陷入坏的区域。